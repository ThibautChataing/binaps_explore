{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2f6edf43-d563-4f7b-ac75-f4064ee9d1f5",
   "metadata": {},
   "source": [
    "# Explore success prediction for github repository\n",
    "\n",
    "ND about data :\n",
    "    * Some repo don't have comit (ex : devanshbatham_FavFreak)\n",
    "    * Some author have multiple mail\n",
    "    * Some mail have multiple author\n",
    "    \n",
    "   -> we take mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c0d958-4027-4191-91ff-fffef673d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5468de-55d3-4c5f-a12b-b66ecd05a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import json\n",
    "from pandas_profiling import ProfileReport\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00fbf5ae-1ff3-48d1-9fd1-0e0ff17c352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(levelname)s : %(message)s', \n",
    "    level=logging.ERROR, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7061f44a-54b3-469b-b708-397dce1d17be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR : error\n"
     ]
    }
   ],
   "source": [
    "logging.debug('debug')\n",
    "logging.warning('warning')\n",
    "logging.error('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6439a250-db31-47e2-ace9-ff06a6d94807",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"./data/\"  # folder holder of json \n",
    "output_dir = \"./output\"  # output folder ready data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1d9063c-689a-4771-a68b-da99ad05881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir(data)\n",
    "df = pd.DataFrame(file_names, columns=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f27cf6-188a-4856-95e1-59f38388c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'] = df.name.str.extract('.*_([A-z]+).json')\n",
    "df['repo_root'] = df.name.str.extract('(.*)_[A-z]+.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eb122fe-dbc3-4ac0-98ba-5635c86f20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[df.type == \"commits\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fdb5baf-e48a-4854-9a1d-8e9c549fe637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commit_from_json(json_file_name : str) -> list:\n",
    "    \"\"\"\n",
    "    Extract date, author id and author email from the commit json file\n",
    "    \"\"\"\n",
    "    logging.debug(f\"Extract commits from {json_file_name}\")\n",
    "    extract_commit_clean = {\n",
    "            'repository': os.path.basename(json_file_name).split('.')[0],\n",
    "            'date': None,\n",
    "            'author_mail': None,\n",
    "            'author_name': None\n",
    "            }\n",
    "    output = []\n",
    "\n",
    "    with open(json_file_name) as fd:\n",
    "        commit_list =  json.load(fd)\n",
    "        failed_err = \"\"\n",
    "        failed_warning =\"\"\n",
    "        for commit in commit_list:\n",
    "            try:\n",
    "                extract_commit = extract_commit_clean.copy()\n",
    "                extract_commit['date'] = commit['commit']['author']['date']\n",
    "                extract_commit['author_mail'] = commit['commit']['author']['email']\n",
    "                extract_commit['author_name'] = commit['commit']['author']['name']\n",
    "            except:\n",
    "                failed_err += f\"Error in finding commit info : {sys.exc_info()[0]}\\n\"\n",
    "            \n",
    "            if extract_commit['author_mail'] == '' and extract_commit['author_mail'] == '':\n",
    "                failed_warning += f\"No info about mail or name of the author in {extract_commit['repository']}\\n\"\n",
    "                continue\n",
    "            elif extract_commit['author_mail'] == '':\n",
    "                extract_commit['author_mail'] = extract_commit['author_name']\n",
    "            elif extract_commit['author_name'] == '':\n",
    "                extract_commit['author_name'] = extract_commit['author_mail']\n",
    "            \n",
    "            output.append(extract_commit)\n",
    "    logging.debug(f\"{len(output)} commits extracted\")\n",
    "    if failed_err:\n",
    "        logging.error(failed_err)\n",
    "    if failed_warning:\n",
    "        logging.warning(failed_warning)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e8ce623-4adb-4f89-b646-a7f25e60d0b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1449783695cf42dc8b45446edc51c1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/592 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_commits = pd.DataFrame(\n",
    "    columns=[\"repository\", \"date\", \"author_mail\", \"author_name\"])\n",
    "for file_name in tqdm(df1[\"name\"]):\n",
    "    file_path = os.path.join(data, file_name)\n",
    "    l_commits = get_commit_from_json(file_path)\n",
    "    df_commits = pd.concat([df_commits, pd.DataFrame.from_records(l_commits)])\n",
    "    \n",
    "df_commits = df_commits.convert_dtypes()\n",
    "df_commits.date = pd.to_datetime(df_commits.date)\n",
    "df_commits.sort_values(\"date\", inplace=True)\n",
    "df_commits.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop the unique commit from 1970\n",
    "df_commits.drop(index=0, axis=0, inplace=True)\n",
    "df_commits.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50b12e6d-6189-4894-84d4-4372e78efadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_commits.to_csv(\"./checkpoint_1_raw_commit_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b071e610-b033-4a13-a64b-e0106a9e748a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#profile = ProfileReport(df_commits, title=\"Pandas Profiling Report\")\n",
    "#profile.to_file(\"commits_profile.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60708581-25b3-47c0-bd36-793f08dfc7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_start = datetime.datetime.strptime(\"2018+0000\", \"%Y%z\")\n",
    "#dfc = df_commits.loc[df_commits.date > time_start]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7f38238-2fb2-4d54-96be-fb46bdba805b",
   "metadata": {},
   "source": [
    "create empty matrice with dim  \n",
    "    col = nbr_author_mail * 7 * 4  \n",
    "\n",
    "get end date of the last commit  \n",
    "remplir la matrice avec :  \n",
    "for each repo :  \n",
    "    get start date of this repo  \n",
    "    from start date to end date with a padding of one day (4 data, we give a value by slice of 6hours)  \n",
    "        create time_start_bound, time_end_bound  \n",
    "        create a new line in df  \n",
    "        if author (id/rank=x) has done a commit  \n",
    "            put 1 in col x+nbr_author*delta_time(commit_date-star_date in hour // 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34049433-b715-4c56-ae86-a9500ed3ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commit_grp_by_repo(time_window_duration: pd.Timedelta, \n",
    "                            smallest_time:int, \n",
    "                            df: pd.DataFrame):\n",
    "\n",
    "    time_binning = int(time_window_duration.total_seconds() / smallest_time) # binning of 6 hours during a week\n",
    "\n",
    "    # Get list of author ordered by total number of commit\n",
    "    df_to_grp = df.copy()\n",
    "    df_to_grp = df_to_grp.reset_index()\n",
    "    authors = df_to_grp.groupby('author_mail').count().sort_values('index', ascending=False).index.to_list()\n",
    "\n",
    "    # create dict to encode author\n",
    "    dict_author = dict(zip(authors, range(len(authors))))\n",
    "    col = []\n",
    "    for i in range(time_binning):\n",
    "        col += [aut + f\"_t_{i}\" for aut in authors]\n",
    "\n",
    "    # Create dataframe structure with columns as author1_t_0 author2_t_0 ... authorN_t_M\n",
    "    #df_sparse = pd.DataFrame(columns=col, dtype=int)\n",
    "    # It's too long to use dataframe, we will use a list of list where we write indice of value of 1\n",
    "    logging.info(f\"{len(col)} columns for the sparse data\")\n",
    "    \n",
    "    df1 = df.copy()\n",
    "    df1 = df1.reset_index()\n",
    "    dfg = df1.set_index(['repository', 'index'])\n",
    "    dfg.sort_index()\n",
    "    \n",
    "\n",
    "    return dfg, dict_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d046a85-c9c4-48e3-a242-4abd18cf7ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compile_input(dfg:pd.DataFrame, time_windows_duration:pd.Timedelta, time_delta, last_commit_date, dict_author, smallest_time):\n",
    "    # From multiindex, get values of index\n",
    "    list_repo = dfg.index.get_level_values(0).to_series().unique().tolist()\n",
    "\n",
    "    meta_list_sparce = []\n",
    "    metadata = []\n",
    "\n",
    "    pbar = tqdm(total=1, desc=\"Time window loop\", leave=True, position=2)\n",
    "\n",
    "    for repo in tqdm(list_repo, desc=\"Repository loop\", leave=True, position=1):\n",
    "        # extract data about only one repo\n",
    "        logging.debug(f\"Filling for {repo}\")\n",
    "        data = dfg.loc[repo]\n",
    "\n",
    "        # get the first commit's date\n",
    "        start_date_repo = data.date.min()\n",
    "        logging.debug(f\"start date = {start_date_repo} or {repo}\")\n",
    "\n",
    "        # get the first boundary for the time window\n",
    "        time_start_boundary = start_date_repo\n",
    "        time_end_boundary = start_date_repo + time_windows_duration\n",
    "\n",
    "        # calculer  le nombre d'itération du while pour ajouter un tqdm(total=max_iter)\n",
    "        tqdm_total = int((last_commit_date - time_start_boundary).total_seconds() / time_delta.total_seconds())\n",
    "        #pbar = tqdm(total=tqdm_total, desc=\"Time window loop\", leave=False, position=1)\n",
    "        pbar.reset(total=tqdm_total)\n",
    "        while(time_end_boundary <= last_commit_date):\n",
    "\n",
    "            list_index_of_1 = []\n",
    "\n",
    "            data_window = data.loc[(data.date >= time_start_boundary) & (data.date < time_end_boundary)]\n",
    "\n",
    "            logging.debug(f\"{len(data_window)} commits founds between {time_start_boundary} and {time_end_boundary} for {repo}\")\n",
    "\n",
    "            for row in data_window.itertuples():\n",
    "\n",
    "                # Get columns_id author_id + which time binning\n",
    "                id_col = dict_author[row.author_mail] + len(dict_author) * int((row.date - time_start_boundary).total_seconds() / smallest_time)\n",
    "\n",
    "                # Set 1 to last row and columns found before\n",
    "                #df_sparse.iloc[-1, id_col] = 1\n",
    "                # don't save duplicate\n",
    "                last_item = -1 if len(list_index_of_1) == 0 else list_index_of_1[-1]\n",
    "                if last_item != id_col:\n",
    "                    list_index_of_1.append(id_col)\n",
    "\n",
    "            if list_index_of_1:\n",
    "                meta_list_sparce.append(list_index_of_1)\n",
    "                metadata.append({\n",
    "                                'id':(len(meta_list_sparce)-1),\n",
    "                                \"repo\":repo,\n",
    "                                \"start_time\":time_start_boundary.strftime(\"%Y-%m-%dT%Hh%Mm%Ss\"),\n",
    "                                \"end_time\":time_end_boundary.strftime(\"%Y-%m-%dT%Hh%Mm%Ss\"),\n",
    "                                }\n",
    "                )\n",
    "            # Increment boundaries with the time delta to move the time window\n",
    "            time_start_boundary += time_delta\n",
    "            time_end_boundary += time_delta\n",
    "\n",
    "            # update progression bar\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    return meta_list_sparce, metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa54ad7e-0116-4363-9b87-a936a661ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(meta_list_sparce, output_dir, name):\n",
    "    file_name = os.path.join(output_dir, f\"{name}.dat\")\n",
    "    with open(file_name, 'w') as fd:\n",
    "        for row in meta_list_sparce:\n",
    "            if len(row) >= 1 :\n",
    "                fd.write(' '.join(map(str, row)))\n",
    "                fd.write('\\n')\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2e00675-c214-4990-a1f6-b657f46a7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata(metadata, output_dir, name):\n",
    "    file_name = os.path.join(output_dir, f\"{name}.txt\")\n",
    "    with open(file_name, 'w') as fd:\n",
    "        json.dump(metadata, fd)\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01455c6e-bf72-44b2-8248-cc5d9373deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat(file_name:str):\n",
    "    maxi = 0\n",
    "    nbr_line = 0\n",
    "    with open(file_name, 'r') as fd:\n",
    "        for line in fd:\n",
    "            if line.strip():\n",
    "                \n",
    "                                # drop newline\n",
    "                l = line[:-1]\n",
    "                if l == \"\":\n",
    "                    continue\n",
    "                if l[-1] == \" \":\n",
    "                    l = l[:-1]\n",
    "                # get indices as array\n",
    "                sl = l.split(\" \")\n",
    "                sl = [int(i) for i in sl]\n",
    "                local_max = max(sl)\n",
    "                    \n",
    "                maxi = maxi if local_max < maxi else local_max\n",
    "                nbr_line += 1\n",
    "            \n",
    "    return nbr_line, maxi\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e783f7de-eb15-4a1f-a3d5-6bc648ced7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_commit_repo_by_week(tmd_i, tmd_t, td_i, td_t, df, smallest_time, suffix:str = \"\", output_dir=\"./\"):\n",
    "    time_windows_duration = pd.Timedelta(tmd_i, tmd_t) # 1 week\n",
    "    time_delta = pd.Timedelta(td_i, td_t) # 1 day \n",
    "\n",
    "    dfg, dict_author = get_commit_grp_by_repo(time_window_duration=time_windows_duration, \n",
    "                                              smallest_time=smallest_time, \n",
    "                                              df=df)\n",
    "\n",
    "    last_commit_date = df.date.max()\n",
    "    logging.info(f\"Latest commit found at {last_commit_date}\")\n",
    "\n",
    "    meta_list_sparce, metadata = compile_input(dfg=dfg, \n",
    "                                     time_windows_duration=time_windows_duration,\n",
    "                                     time_delta=time_delta, \n",
    "                                     last_commit_date=last_commit_date, \n",
    "                                     smallest_time=smallest_time,\n",
    "                                     dict_author=dict_author,)\n",
    "\n",
    "    today = datetime.datetime.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss\")\n",
    "    name = f\"github_cyber_{today}__{tmd_i}{tmd_t}_spaced_{td_i}{td_t}_smallest_{smallest_time/3600}\"+suffix\n",
    "    file_name = save_data(meta_list_sparce, output_dir, name)\n",
    "    file_meta = save_metadata(metadata, output_dir, name)\n",
    "    nbr_row, nbr_feature = get_stat(file_name)\n",
    "    \n",
    "    logging.info(f\"{nbr_feature} features and {nbr_row} rows for {file_name}\")\n",
    "    return nbr_row, nbr_feature, file_name, file_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "886e97a4-ccb0-4a01-b8ab-643fcf95d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling(data, nbr_of_sample=10,log=False):\n",
    "    \"\"\"\n",
    "    Get sampling from data, uniform or log\n",
    "    \"\"\"\n",
    "    repos = data.groupby('repository').count().sort_values('date', ascending=False).index.to_series().reset_index(drop=True)\n",
    "    if log:\n",
    "        index = gen_log_space(repos.index.max(), nbr_of_sample)\n",
    "        repos_name = repos.iloc[index]\n",
    "    else:\n",
    "        repos_name = repos.sample(nbr_of_sample)\n",
    "\n",
    "    logging.debug(f\"Sampling {repos_name}\")\n",
    "    \n",
    "    ret = data.loc[data.repository.isin(repos_name)]\n",
    "    \n",
    "    logging.info(f\"From {len(data)} to {len(ret)} samples\")\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "787c1e6e-41ef-4793-943a-804302c2f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_log_space(limit, n):\n",
    "    \"\"\"\n",
    "    limit (int) : max number to have\n",
    "    n : how many sample\n",
    "    \"\"\"\n",
    "    \n",
    "    result = [1]\n",
    "    if n>1:  # just a check to avoid ZeroDivisionError\n",
    "        ratio = (float(limit)/result[-1]) ** (1.0/(n-len(result)))\n",
    "    while len(result)<n:\n",
    "        next_value = result[-1]*ratio\n",
    "        if next_value - result[-1] >= 1:\n",
    "            # safe zone. next_value will be a different integer\n",
    "            result.append(next_value)\n",
    "        else:\n",
    "            # problem! same integer. we need to find next_value by artificially incrementing previous value\n",
    "            result.append(result[-1]+1)\n",
    "            # recalculate the ratio so that the remaining values will scale correctly\n",
    "            ratio = (float(limit)/result[-1]) ** (1.0/(n-len(result)))\n",
    "    # round, re-adjust to 0 indexing (i.e. minus 1) and return np.uint64 array\n",
    "    return np.array(list(map(lambda x: round(x)-1, result)), dtype=np.uint64)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38d82c68-dc65-4fdc-af9d-5e308b762fa7",
   "metadata": {},
   "source": [
    "report :\n",
    "    - Nbr of line (number of line in output)\n",
    "    - Nbr of feature (max of author)\n",
    "    \n",
    "ND : beaucoup de ligne vide, peut être que la fenetre d'étude est trop courte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "663895da-e405-4732-81c2-4e5be06591a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43780b0065547f58154870668c45834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "List of arguments:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 16:05:43,070 INFO : Arg : (1, 'w', 1, 'd', 21600, 50, False)\n",
      "2022-07-22 16:05:43,301 INFO : From 599881 to 99165 samples\n",
      "2022-07-22 16:05:43,406 INFO : 51716 columns for the sparse data\n",
      "2022-07-22 16:05:43,509 INFO : Latest commit found at 2022-04-21 10:20:59+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4501d25713c84ee0a85d49bbe58afe24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b973d05d80049a3a56596cee1887631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 16:07:50,529 INFO : 51715 features and 47948 rows for ./output/github_cyber_2022-07-22T16h07m49s__1w_spaced_1d_smallest_6.0_50uni.dat\n",
      "2022-07-22 16:07:50,547 INFO : Rapport = (1, 'w', 1, 'd', 21600, 50, False) :  rows=47948, lines=51715, data=./output/github_cyber_2022-07-22T16h07m49s__1w_spaced_1d_smallest_6.0_50uni.dat, meta=./output/github_cyber_2022-07-22T16h07m49s__1w_spaced_1d_smallest_6.0_50uni.txt\n",
      "\n",
      "2022-07-22 16:07:50,651 INFO : Arg : (1, 'w', 1, 'd', 21600, 50, True)\n",
      "2022-07-22 16:07:50,938 INFO : From 599881 to 349190 samples\n",
      "2022-07-22 16:07:51,261 INFO : 164584 columns for the sparse data\n",
      "2022-07-22 16:07:51,651 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a9859af9a641c5a9a517dc6459bd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c36ce8708044cf4ad8019b5bccfbaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 16:10:57,271 INFO : 164570 features and 96390 rows for ./output/github_cyber_2022-07-22T16h10m55s__1w_spaced_1d_smallest_6.0_50log.dat\n",
      "2022-07-22 16:10:57,342 INFO : Rapport = (1, 'w', 1, 'd', 21600, 50, True) :  rows=96390, lines=164570, data=./output/github_cyber_2022-07-22T16h10m55s__1w_spaced_1d_smallest_6.0_50log.dat, meta=./output/github_cyber_2022-07-22T16h10m55s__1w_spaced_1d_smallest_6.0_50log.txt\n",
      "\n",
      "2022-07-22 16:10:57,464 INFO : Arg : (1, 'w', 1, 'd', 86400, 50, False)\n",
      "2022-07-22 16:10:57,689 INFO : From 599881 to 44083 samples\n",
      "2022-07-22 16:10:57,737 INFO : 8400 columns for the sparse data\n",
      "2022-07-22 16:10:57,785 INFO : Latest commit found at 2022-04-21 12:46:47+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471aad46536342129d42e0fd35d6fdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f460190bfd84347b0b2a8e871e68026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 16:12:44,404 INFO : 8399 features and 30027 rows for ./output/github_cyber_2022-07-22T16h12m44s__1w_spaced_1d_smallest_24.0_50uni.dat\n",
      "2022-07-22 16:12:44,415 INFO : Rapport = (1, 'w', 1, 'd', 86400, 50, False) :  rows=30027, lines=8399, data=./output/github_cyber_2022-07-22T16h12m44s__1w_spaced_1d_smallest_24.0_50uni.dat, meta=./output/github_cyber_2022-07-22T16h12m44s__1w_spaced_1d_smallest_24.0_50uni.txt\n",
      "\n",
      "2022-07-22 16:12:44,506 INFO : Arg : (1, 'w', 1, 'd', 86400, 50, True)\n",
      "2022-07-22 16:12:44,760 INFO : From 599881 to 349190 samples\n",
      "2022-07-22 16:12:45,043 INFO : 41146 columns for the sparse data\n",
      "2022-07-22 16:12:45,403 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943732b2ff5642d783614433da1ea22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574cab680cc949b789071779f4ba9dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 16:15:47,923 INFO : 41145 features and 96390 rows for ./output/github_cyber_2022-07-22T16h15m46s__1w_spaced_1d_smallest_24.0_50log.dat\n",
      "2022-07-22 16:15:47,969 INFO : Rapport = (1, 'w', 1, 'd', 86400, 50, True) :  rows=96390, lines=41145, data=./output/github_cyber_2022-07-22T16h15m46s__1w_spaced_1d_smallest_24.0_50log.dat, meta=./output/github_cyber_2022-07-22T16h15m46s__1w_spaced_1d_smallest_24.0_50log.txt\n",
      "\n",
      "2022-07-22 16:15:48,058 INFO : Arg : (1, 'w', 2, 'd', 21600, 50, False)\n",
      "2022-07-22 16:15:48,270 INFO : From 599881 to 40377 samples\n",
      "2022-07-22 16:15:48,317 INFO : 30548 columns for the sparse data\n",
      "2022-07-22 16:15:48,360 INFO : Latest commit found at 2022-04-21 12:46:47+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21a59ed1cbc405297d4c3995d5aa7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3a2e4118224eeea6ff862e41d80a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 16:16:37,621 INFO : 30538 features and 14860 rows for ./output/github_cyber_2022-07-22T16h16m37s__1w_spaced_2d_smallest_6.0_50uni.dat\n",
      "2022-07-22 16:16:37,628 INFO : Rapport = (1, 'w', 2, 'd', 21600, 50, False) :  rows=14860, lines=30538, data=./output/github_cyber_2022-07-22T16h16m37s__1w_spaced_2d_smallest_6.0_50uni.dat, meta=./output/github_cyber_2022-07-22T16h16m37s__1w_spaced_2d_smallest_6.0_50uni.txt\n",
      "\n",
      "2022-07-22 16:16:37,728 INFO : Arg : (1, 'w', 2, 'd', 21600, 50, True)\n",
      "2022-07-22 16:16:37,989 INFO : From 599881 to 349190 samples\n",
      "2022-07-22 16:16:38,299 INFO : 164584 columns for the sparse data\n",
      "2022-07-22 16:16:38,659 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1f63635ba24264ba69f4dc6d40ef6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9ed33f0c034281982b79f93c91e789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 16:18:10,254 INFO : 164570 features and 48215 rows for ./output/github_cyber_2022-07-22T16h18m09s__1w_spaced_2d_smallest_6.0_50log.dat\n",
      "2022-07-22 16:18:10,284 INFO : Rapport = (1, 'w', 2, 'd', 21600, 50, True) :  rows=48215, lines=164570, data=./output/github_cyber_2022-07-22T16h18m09s__1w_spaced_2d_smallest_6.0_50log.dat, meta=./output/github_cyber_2022-07-22T16h18m09s__1w_spaced_2d_smallest_6.0_50log.txt\n",
      "\n",
      "2022-07-22 16:18:10,378 INFO : Arg : (1, 'w', 2, 'd', 86400, 50, False)\n",
      "2022-07-22 16:18:10,593 INFO : From 599881 to 52714 samples\n",
      "2022-07-22 16:18:10,648 INFO : 13426 columns for the sparse data\n",
      "2022-07-22 16:18:10,704 INFO : Latest commit found at 2022-04-19 22:04:06+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7dc60076bd4efb8460cbea44c2c43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfabcad00583491bb8950e72fa517fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 16:19:01,207 INFO : 13425 features and 15234 rows for ./output/github_cyber_2022-07-22T16h19m01s__1w_spaced_2d_smallest_24.0_50uni.dat\n",
      "2022-07-22 16:19:01,219 INFO : Rapport = (1, 'w', 2, 'd', 86400, 50, False) :  rows=15234, lines=13425, data=./output/github_cyber_2022-07-22T16h19m01s__1w_spaced_2d_smallest_24.0_50uni.dat, meta=./output/github_cyber_2022-07-22T16h19m01s__1w_spaced_2d_smallest_24.0_50uni.txt\n",
      "\n",
      "2022-07-22 16:19:01,326 INFO : Arg : (1, 'w', 2, 'd', 86400, 50, True)\n",
      "2022-07-22 16:19:01,621 INFO : From 599881 to 349190 samples\n",
      "2022-07-22 16:19:01,913 INFO : 41146 columns for the sparse data\n",
      "2022-07-22 16:19:02,271 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efea03877ff04b8cb155973d6e9e5a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1d42bb6c4146f6916a2d3bcf22f7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 16:20:33,859 INFO : 41145 features and 48215 rows for ./output/github_cyber_2022-07-22T16h20m33s__1w_spaced_2d_smallest_24.0_50log.dat\n",
      "2022-07-22 16:20:33,887 INFO : Rapport = (1, 'w', 2, 'd', 86400, 50, True) :  rows=48215, lines=41145, data=./output/github_cyber_2022-07-22T16h20m33s__1w_spaced_2d_smallest_24.0_50log.dat, meta=./output/github_cyber_2022-07-22T16h20m33s__1w_spaced_2d_smallest_24.0_50log.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                    handlers=[logging.FileHandler(\"my_log.log\", mode='w'),\n",
    "                              stream_handler],\n",
    "                   force=True)\n",
    "\n",
    "\n",
    "tmd_i = [1]  # number of tmd_t\n",
    "tmd_t = ['w']  # type of tmd_i\n",
    "\n",
    "# Definition of time delta between two time window\n",
    "td_i = [1,2]  # nomber of td_t\n",
    "td_t = ['d']  # type of td_i\n",
    "\n",
    "smallest_time = [6*3600, 24*3600] # 6 hours in seconds\n",
    "\n",
    "sampling = [50]\n",
    "log = [False, True]\n",
    "\n",
    "args = list(itertools.product(*[tmd_i, tmd_t, td_i, td_t, smallest_time, sampling, log]))\n",
    "\n",
    "output_dir = \"./output\"\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss\")\n",
    "\n",
    "for arg in tqdm(args, desc=\"List of arguments\", position=0):\n",
    "    if arg[5] == -1 and arg[6]:\n",
    "        continue\n",
    "        \n",
    "    df = df_commits.copy()\n",
    "    \n",
    "    logging.info(f\"Arg : {arg}\")\n",
    "    rapport = f\"{arg} : \"\n",
    "    suffix=\"\"\n",
    "    if arg[5] > 0 :\n",
    "        dfs = get_sampling(df, arg[5], log=arg[6])\n",
    "        suffix=f\"_{arg[5]}\"\n",
    "        if arg[6]:\n",
    "            suffix += \"log\"\n",
    "        else:\n",
    "            suffix += 'uni'  \n",
    "    else :\n",
    "        dfs = df.copy()\n",
    "    rows, lines, file_name, file_meta = from_commit_repo_by_week(df=dfs,\n",
    "                                         tmd_i=arg[0], \n",
    "                                         tmd_t=arg[1], \n",
    "                                         td_i=arg[2], \n",
    "                                         td_t=arg[3], \n",
    "                                         smallest_time=arg[4], \n",
    "                                         output_dir=output_dir ,\n",
    "                                         suffix=suffix)\n",
    "    rapport += f\" rows={rows}, lines={lines}, data={file_name}, meta={file_meta}\\n\"\n",
    "    logging.info(f\"Rapport = {rapport}\")\n",
    "    with open(os.path.join(output_dir, f\"rapport_{today}.txt\"), 'a+') as fd:\n",
    "              fd.write(rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f13ab-9af5-4ddc-8d2e-a2f094b6d2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
