{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2f6edf43-d563-4f7b-ac75-f4064ee9d1f5",
   "metadata": {},
   "source": [
    "# Explore success prediction for github repository\n",
    "\n",
    "ND about data :\n",
    "    * Some repo don't have comit (ex : devanshbatham_FavFreak)\n",
    "    * Some author have multiple mail\n",
    "    * Some mail have multiple author\n",
    "    \n",
    "   -> we take mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c0d958-4027-4191-91ff-fffef673d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b5468de-55d3-4c5f-a12b-b66ecd05a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from pandas_profiling import ProfileReport\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fbf5ae-1ff3-48d1-9fd1-0e0ff17c352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(levelname)s : %(message)s', \n",
    "    level=logging.ERROR, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7061f44a-54b3-469b-b708-397dce1d17be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR : error\n"
     ]
    }
   ],
   "source": [
    "logging.debug('debug')\n",
    "logging.warning('warning')\n",
    "logging.error('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6439a250-db31-47e2-ace9-ff06a6d94807",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"./data/\"  # folder holder of json \n",
    "output_dir = \"./output\"  # output folder ready data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d9063c-689a-4771-a68b-da99ad05881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir(data)\n",
    "df = pd.DataFrame(file_names, columns=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f27cf6-188a-4856-95e1-59f38388c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'] = df.name.str.extract('.*_([A-z]+).json')\n",
    "df['repo_root'] = df.name.str.extract('(.*)_[A-z]+.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb122fe-dbc3-4ac0-98ba-5635c86f20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[df.type == \"commits\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fdb5baf-e48a-4854-9a1d-8e9c549fe637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commit_from_json(json_file_name : str) -> list:\n",
    "    \"\"\"\n",
    "    Extract date, author id and author email from the commit json file\n",
    "    \"\"\"\n",
    "    logging.debug(f\"Extract commits from {json_file_name}\")\n",
    "    extract_commit_clean = {\n",
    "            'repository': os.path.basename(json_file_name).split('.')[0],\n",
    "            'date': None,\n",
    "            'author_mail': None,\n",
    "            'author_name': None\n",
    "            }\n",
    "    output = []\n",
    "\n",
    "    with open(json_file_name) as fd:\n",
    "        commit_list =  json.load(fd)\n",
    "        failed_err = \"\"\n",
    "        failed_warning =\"\"\n",
    "        for commit in commit_list:\n",
    "            try:\n",
    "                extract_commit = extract_commit_clean.copy()\n",
    "                extract_commit['date'] = commit['commit']['author']['date']\n",
    "                extract_commit['author_mail'] = commit['commit']['author']['email']\n",
    "                extract_commit['author_name'] = commit['commit']['author']['name']\n",
    "            except:\n",
    "                failed_err += f\"Error in finding commit info : {sys.exc_info()[0]}\\n\"\n",
    "            \n",
    "            if extract_commit['author_mail'] == '' and extract_commit['author_mail'] == '':\n",
    "                failed_warning += f\"No info about mail or name of the author in {extract_commit['repository']}\\n\"\n",
    "                continue\n",
    "            elif extract_commit['author_mail'] == '':\n",
    "                extract_commit['author_mail'] = extract_commit['author_name']\n",
    "            elif extract_commit['author_name'] == '':\n",
    "                extract_commit['author_name'] = extract_commit['author_mail']\n",
    "            \n",
    "            output.append(extract_commit)\n",
    "    logging.debug(f\"{len(output)} commits extracted\")\n",
    "    if failed_err:\n",
    "        logging.error(failed_err)\n",
    "    if failed_warning:\n",
    "        logging.warning(failed_warning)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e8ce623-4adb-4f89-b646-a7f25e60d0b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49e9ff12717462b8d1aa59cad6831d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/592 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_commits = pd.DataFrame(\n",
    "    columns=[\"repository\", \"date\", \"author_mail\", \"author_name\"])\n",
    "for file_name in tqdm(df1[\"name\"]):\n",
    "    file_path = os.path.join(data, file_name)\n",
    "    l_commits = get_commit_from_json(file_path)\n",
    "    df_commits = pd.concat([df_commits, pd.DataFrame.from_records(l_commits)])\n",
    "    \n",
    "df_commits = df_commits.convert_dtypes()\n",
    "df_commits.date = pd.to_datetime(df_commits.date)\n",
    "df_commits.sort_values(\"date\", inplace=True)\n",
    "df_commits.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop the unique commit from 1970\n",
    "df_commits.drop(index=0, axis=0, inplace=True)\n",
    "df_commits.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b12e6d-6189-4894-84d4-4372e78efadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_commits.to_csv(\"./checkpoint_1_raw_commit_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b071e610-b033-4a13-a64b-e0106a9e748a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#profile = ProfileReport(df_commits, title=\"Pandas Profiling Report\")\n",
    "#profile.to_file(\"commits_profile.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60708581-25b3-47c0-bd36-793f08dfc7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_start = datetime.datetime.strptime(\"2018+0000\", \"%Y%z\")\n",
    "#dfc = df_commits.loc[df_commits.date > time_start]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7f38238-2fb2-4d54-96be-fb46bdba805b",
   "metadata": {},
   "source": [
    "create empty matrice with dim  \n",
    "    col = nbr_author_mail * 7 * 4  \n",
    "\n",
    "get end date of the last commit  \n",
    "remplir la matrice avec :  \n",
    "for each repo :  \n",
    "    get start date of this repo  \n",
    "    from start date to end date with a padding of one day (4 data, we give a value by slice of 6hours)  \n",
    "        create time_start_bound, time_end_bound  \n",
    "        create a new line in df  \n",
    "        if author (id/rank=x) has done a commit  \n",
    "            put 1 in col x+nbr_author*delta_time(commit_date-star_date in hour // 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34049433-b715-4c56-ae86-a9500ed3ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commit_grp_by_repo(time_window_duration: pd.Timedelta, \n",
    "                            smallest_time:int, \n",
    "                            df: pd.DataFrame):\n",
    "\n",
    "    time_binning = int(time_window_duration.total_seconds() / smallest_time) # binning of 6 hours during a week\n",
    "\n",
    "    # Get list of author ordered by total number of commit\n",
    "    df_to_grp = df.copy()\n",
    "    df_to_grp = df_to_grp.reset_index()\n",
    "    authors = df_to_grp.groupby('author_mail').count().sort_values('index', ascending=False).index.to_list()\n",
    "\n",
    "    # create dict to encode author\n",
    "    dict_author = dict(zip(authors, range(len(authors))))\n",
    "    col = []\n",
    "    for i in range(time_binning):\n",
    "        col += [aut + f\"_t_{i}\" for aut in authors]\n",
    "\n",
    "    # Create dataframe structure with columns as author1_t_0 author2_t_0 ... authorN_t_M\n",
    "    #df_sparse = pd.DataFrame(columns=col, dtype=int)\n",
    "    # It's too long to use dataframe, we will use a list of list where we write indice of value of 1\n",
    "    logging.info(f\"{len(col)} columns for the sparse data\")\n",
    "    \n",
    "    df1 = df.copy()\n",
    "    df1 = df1.reset_index()\n",
    "    dfg = df1.set_index(['repository', 'index'])\n",
    "    dfg.sort_index()\n",
    "    \n",
    "\n",
    "    return dfg, dict_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d046a85-c9c4-48e3-a242-4abd18cf7ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compile_input(dfg:pd.DataFrame, time_windows_duration:pd.Timedelta, time_delta, last_commit_date, dict_author, smallest_time):\n",
    "    # From multiindex, get values of index\n",
    "    list_repo = dfg.index.get_level_values(0).to_series().unique().tolist()\n",
    "\n",
    "    meta_list_sparce = []\n",
    "\n",
    "    pbar = tqdm(total=1, desc=\"Time window loop\", leave=True, position=2)\n",
    "\n",
    "    for repo in tqdm(list_repo, desc=\"Repository loop\", leave=True, position=1):\n",
    "        # extract data about only one repo\n",
    "        logging.debug(f\"Filling for {repo}\")\n",
    "        data = dfg.loc[repo]\n",
    "\n",
    "        # get the first commit's date\n",
    "        start_date_repo = data.date.min()\n",
    "        logging.debug(f\"start date = {start_date_repo} or {repo}\")\n",
    "\n",
    "        # get the first boundary for the time window\n",
    "        time_start_boundary = start_date_repo\n",
    "        time_end_boundary = start_date_repo + time_windows_duration\n",
    "\n",
    "        # calculer  le nombre d'itération du while pour ajouter un tqdm(total=max_iter)\n",
    "        tqdm_total = int((last_commit_date - time_start_boundary).total_seconds() / time_delta.total_seconds())\n",
    "        #pbar = tqdm(total=tqdm_total, desc=\"Time window loop\", leave=False, position=1)\n",
    "        pbar.reset(total=tqdm_total)\n",
    "        while(time_end_boundary <= last_commit_date):\n",
    "\n",
    "            # Fill a row with zeros\n",
    "            #df_sparse.loc[len(df_sparse)] = pd.NA\n",
    "            list_index_of_1 = []\n",
    "\n",
    "            data_window = data.loc[(data.date >= time_start_boundary) & (data.date < time_end_boundary)]\n",
    "\n",
    "            logging.debug(f\"{len(data_window)} commits founds between {time_start_boundary} and {time_end_boundary} for {repo}\")\n",
    "\n",
    "            for row in data_window.itertuples():\n",
    "\n",
    "                # Get columns_id author_id + which time binning\n",
    "                id_col = dict_author[row.author_mail] + len(dict_author) * int((row.date - time_start_boundary).total_seconds() / smallest_time)\n",
    "\n",
    "                # Set 1 to last row and columns found before\n",
    "                #df_sparse.iloc[-1, id_col] = 1\n",
    "                # don't save duplicate\n",
    "                last_item = -1 if len(list_index_of_1) == 0 else list_index_of_1[-1]\n",
    "                if last_item != id_col:\n",
    "                    list_index_of_1.append(id_col)\n",
    "\n",
    "            meta_list_sparce.append(list_index_of_1)\n",
    "            # Increment boundaries with the time delta to move the time window\n",
    "            time_start_boundary += time_delta\n",
    "            time_end_boundary += time_delta\n",
    "\n",
    "            # update progression bar\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    return meta_list_sparce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa54ad7e-0116-4363-9b87-a936a661ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(meta_list_sparce, output_dir, suffix):\n",
    "    today = datetime.datetime.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss\")\n",
    "    file_name = os.path.join(output_dir, f\"github_cyber_{today}_{suffix}.dat\")\n",
    "    with open(file_name, 'w') as fd:\n",
    "        for row in meta_list_sparce:\n",
    "            if len(row) >= 1 :\n",
    "                fd.write(' '.join(map(str, row)))\n",
    "                fd.write('\\n')\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01455c6e-bf72-44b2-8248-cc5d9373deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat(file_name:str):\n",
    "    maxi = 0\n",
    "    nbr_line = 0\n",
    "    with open(file_name, 'r') as fd:\n",
    "        for line in fd:\n",
    "            if line.strip():\n",
    "                \n",
    "                                # drop newline\n",
    "                l = line[:-1]\n",
    "                if l == \"\":\n",
    "                    continue\n",
    "                if l[-1] == \" \":\n",
    "                    l = l[:-1]\n",
    "                # get indices as array\n",
    "                sl = l.split(\" \")\n",
    "                sl = [int(i) for i in sl]\n",
    "                local_max = max(sl)\n",
    "                    \n",
    "                maxi = maxi if local_max < maxi else local_max\n",
    "                nbr_line += 1\n",
    "            \n",
    "    return nbr_line, maxi\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e783f7de-eb15-4a1f-a3d5-6bc648ced7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_commit_repo_by_week(tmd_i, tmd_t, td_i, td_t, df, smallest_time, suffix:str = \"\", output_dir=\"./\"):\n",
    "    time_windows_duration = pd.Timedelta(tmd_i, tmd_t) # 1 week\n",
    "    time_delta = pd.Timedelta(td_i, td_t) # 1 day \n",
    "\n",
    "    dfg, dict_author = get_commit_grp_by_repo(time_window_duration=time_windows_duration, \n",
    "                                              smallest_time=smallest_time, \n",
    "                                              df=df)\n",
    "\n",
    "    last_commit_date = df.date.max()\n",
    "    logging.info(f\"Latest commit found at {last_commit_date}\")\n",
    "\n",
    "    meta_list_sparce = compile_input(dfg=dfg, \n",
    "                                     time_windows_duration=time_windows_duration,\n",
    "                                     time_delta=time_delta, \n",
    "                                     last_commit_date=last_commit_date, \n",
    "                                     smallest_time=smallest_time,\n",
    "                                     dict_author=dict_author,)\n",
    "\n",
    "    file_name = save_data(meta_list_sparce, output_dir, f\"_{tmd_i}{tmd_t}_spaced_{td_i}{td_t}_smallest_{smallest_time/3600}\"+suffix)\n",
    "    nbr_row, nbr_feature = get_stat(file_name)\n",
    "    \n",
    "    logging.info(f\"{nbr_feature} features and {nbr_row} rows for {file_name}\")\n",
    "    return nbr_row, nbr_feature, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "886e97a4-ccb0-4a01-b8ab-643fcf95d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling(data, nbr_of_sample=10,log=False):\n",
    "    \"\"\"\n",
    "    Get sampling from data, uniform or log\n",
    "    \"\"\"\n",
    "    repos = data.groupby('repository').count().sort_values('date', ascending=False).index.to_series().reset_index(drop=True)\n",
    "    if log:\n",
    "        index = gen_log_space(repos.index.max(), nbr_of_sample)\n",
    "        repos_name = repos.iloc[index]\n",
    "    else:\n",
    "        repos_name = repos.sample(nbr_of_sample)\n",
    "\n",
    "    logging.debug(f\"Sampling {repos_name}\")\n",
    "    \n",
    "    ret = data.loc[data.repository.isin(repos_name)]\n",
    "    \n",
    "    logging.info(f\"From {len(data)} to {len(ret)} samples\")\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "787c1e6e-41ef-4793-943a-804302c2f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_log_space(limit, n):\n",
    "    \"\"\"\n",
    "    limit (int) : max number to have\n",
    "    n : how many sample\n",
    "    \"\"\"\n",
    "    \n",
    "    result = [1]\n",
    "    if n>1:  # just a check to avoid ZeroDivisionError\n",
    "        ratio = (float(limit)/result[-1]) ** (1.0/(n-len(result)))\n",
    "    while len(result)<n:\n",
    "        next_value = result[-1]*ratio\n",
    "        if next_value - result[-1] >= 1:\n",
    "            # safe zone. next_value will be a different integer\n",
    "            result.append(next_value)\n",
    "        else:\n",
    "            # problem! same integer. we need to find next_value by artificially incrementing previous value\n",
    "            result.append(result[-1]+1)\n",
    "            # recalculate the ratio so that the remaining values will scale correctly\n",
    "            ratio = (float(limit)/result[-1]) ** (1.0/(n-len(result)))\n",
    "    # round, re-adjust to 0 indexing (i.e. minus 1) and return np.uint64 array\n",
    "    return np.array(list(map(lambda x: round(x)-1, result)), dtype=np.uint64)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38d82c68-dc65-4fdc-af9d-5e308b762fa7",
   "metadata": {},
   "source": [
    "report :\n",
    "    - Nbr of line (number of line in output)\n",
    "    - Nbr of feature (max of author)\n",
    "    \n",
    "ND : beaucoup de ligne vide, peut être que la fenetre d'étude est trop courte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "663895da-e405-4732-81c2-4e5be06591a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ff2252bd7a4a7e9cea2767c09c9e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "List of arguments:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 12:13:15,609 INFO : Arg : (1, 'w', 1, 'd', 21600, -1, False)\n",
      "2022-07-17 12:13:16,397 INFO : 361676 columns for the sparse data\n",
      "2022-07-17 12:13:17,217 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fdc7f7b59a44d6944b071da9767007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a638750ea2694de997008eeaf9aeea12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 12:29:07,685 INFO : 361674 features and 310668 rows for ./output/github_cyber_2022-07-17T12h29m06s__1w_spaced_1d_smallest_6.0.dat\n",
      "2022-07-17 12:29:07,769 INFO : Rapport = (1, 'w', 1, 'd', 21600, -1, False) :  rows=310668, lines=361674, file=./output/github_cyber_2022-07-17T12h29m06s__1w_spaced_1d_smallest_6.0.dat\n",
      "\n",
      "2022-07-17 12:29:07,877 INFO : Arg : (1, 'w', 1, 'd', 21600, 50, False)\n",
      "2022-07-17 12:29:08,151 INFO : From 599881 to 119083 samples\n",
      "2022-07-17 12:29:08,312 INFO : 78036 columns for the sparse data\n",
      "2022-07-17 12:29:08,459 INFO : Latest commit found at 2022-04-20 18:51:53+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2253ea824b4a494089a70d34dd94aa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bc63826e0344b3902c591deb3bccc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 12:31:02,214 INFO : 78032 features and 36015 rows for ./output/github_cyber_2022-07-17T12h31m01s__1w_spaced_1d_smallest_6.0_50uni.dat\n",
      "2022-07-17 12:31:02,229 INFO : Rapport = (1, 'w', 1, 'd', 21600, 50, False) :  rows=36015, lines=78032, file=./output/github_cyber_2022-07-17T12h31m01s__1w_spaced_1d_smallest_6.0_50uni.dat\n",
      "\n",
      "2022-07-17 12:31:02,336 INFO : Arg : (1, 'w', 1, 'd', 21600, 50, True)\n",
      "2022-07-17 12:31:02,669 INFO : From 599881 to 349190 samples\n",
      "2022-07-17 12:31:03,072 INFO : 164584 columns for the sparse data\n",
      "2022-07-17 12:31:03,543 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fb40ba19c14ec7a44f1cf40530fc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c336f6324bd54691bd191e7be9226ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 12:34:11,810 INFO : 164570 features and 96390 rows for ./output/github_cyber_2022-07-17T12h34m11s__1w_spaced_1d_smallest_6.0_50log.dat\n",
      "2022-07-17 12:34:11,842 INFO : Rapport = (1, 'w', 1, 'd', 21600, 50, True) :  rows=96390, lines=164570, file=./output/github_cyber_2022-07-17T12h34m11s__1w_spaced_1d_smallest_6.0_50log.dat\n",
      "\n",
      "2022-07-17 12:34:11,946 INFO : Arg : (1, 'w', 1, 'd', 86400, -1, False)\n",
      "2022-07-17 12:34:12,660 INFO : 90419 columns for the sparse data\n",
      "2022-07-17 12:34:13,465 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d2f0d0bab1491f8dce5dddafe9870c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a62663635345aaac48a67aa873368e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 12:50:46,753 INFO : 90418 features and 310668 rows for ./output/github_cyber_2022-07-17T12h50m45s__1w_spaced_1d_smallest_24.0.dat\n",
      "2022-07-17 12:50:46,832 INFO : Rapport = (1, 'w', 1, 'd', 86400, -1, False) :  rows=310668, lines=90418, file=./output/github_cyber_2022-07-17T12h50m45s__1w_spaced_1d_smallest_24.0.dat\n",
      "\n",
      "2022-07-17 12:50:46,936 INFO : Arg : (1, 'w', 1, 'd', 86400, 50, False)\n",
      "2022-07-17 12:50:47,213 INFO : From 599881 to 87741 samples\n",
      "2022-07-17 12:50:47,325 INFO : 11753 columns for the sparse data\n",
      "2022-07-17 12:50:47,430 INFO : Latest commit found at 2022-04-22 16:36:26+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c984e38ffb7f42bb88ae43d7dd858b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ff8b1b01944b34b367f000f72a4691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 12:52:36,287 INFO : 11752 features and 38782 rows for ./output/github_cyber_2022-07-17T12h52m36s__1w_spaced_1d_smallest_24.0_50uni.dat\n",
      "2022-07-17 12:52:36,303 INFO : Rapport = (1, 'w', 1, 'd', 86400, 50, False) :  rows=38782, lines=11752, file=./output/github_cyber_2022-07-17T12h52m36s__1w_spaced_1d_smallest_24.0_50uni.dat\n",
      "\n",
      "2022-07-17 12:52:36,407 INFO : Arg : (1, 'w', 1, 'd', 86400, 50, True)\n",
      "2022-07-17 12:52:36,705 INFO : From 599881 to 349190 samples\n",
      "2022-07-17 12:52:37,045 INFO : 41146 columns for the sparse data\n",
      "2022-07-17 12:52:37,475 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f042014593aa40e1abeca27ff56f4193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05964f8d1cde4a2a8813489495fd6506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 12:55:51,177 INFO : 41145 features and 96390 rows for ./output/github_cyber_2022-07-17T12h55m50s__1w_spaced_1d_smallest_24.0_50log.dat\n",
      "2022-07-17 12:55:51,210 INFO : Rapport = (1, 'w', 1, 'd', 86400, 50, True) :  rows=96390, lines=41145, file=./output/github_cyber_2022-07-17T12h55m50s__1w_spaced_1d_smallest_24.0_50log.dat\n",
      "\n",
      "2022-07-17 12:55:51,319 INFO : Arg : (1, 'w', 2, 'd', 21600, -1, False)\n",
      "2022-07-17 12:55:52,158 INFO : 361676 columns for the sparse data\n",
      "2022-07-17 12:55:52,988 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f9758e03794868b137f08034ca1385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7126d8a672a14a8ba7314fa205d5f8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 13:03:50,849 INFO : 361673 features and 155567 rows for ./output/github_cyber_2022-07-17T13h03m50s__1w_spaced_2d_smallest_6.0.dat\n",
      "2022-07-17 13:03:50,902 INFO : Rapport = (1, 'w', 2, 'd', 21600, -1, False) :  rows=155567, lines=361673, file=./output/github_cyber_2022-07-17T13h03m50s__1w_spaced_2d_smallest_6.0.dat\n",
      "\n",
      "2022-07-17 13:03:51,004 INFO : Arg : (1, 'w', 2, 'd', 21600, 50, False)\n",
      "2022-07-17 13:03:51,257 INFO : From 599881 to 56612 samples\n",
      "2022-07-17 13:03:51,335 INFO : 31052 columns for the sparse data\n",
      "2022-07-17 13:03:51,404 INFO : Latest commit found at 2022-04-21 06:19:04+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6601d3d6f55e41d0976eae0516945cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5874a993fb0498ba34042932834633d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 13:04:47,652 INFO : 31044 features and 21501 rows for ./output/github_cyber_2022-07-17T13h04m47s__1w_spaced_2d_smallest_6.0_50uni.dat\n",
      "2022-07-17 13:04:47,661 INFO : Rapport = (1, 'w', 2, 'd', 21600, 50, False) :  rows=21501, lines=31044, file=./output/github_cyber_2022-07-17T13h04m47s__1w_spaced_2d_smallest_6.0_50uni.dat\n",
      "\n",
      "2022-07-17 13:04:47,764 INFO : Arg : (1, 'w', 2, 'd', 21600, 50, True)\n",
      "2022-07-17 13:04:48,063 INFO : From 599881 to 349190 samples\n",
      "2022-07-17 13:04:48,447 INFO : 164584 columns for the sparse data\n",
      "2022-07-17 13:04:48,876 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349f07f75028463f91c888636496265f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa3662b73c94d52abff84597f887be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 13:06:18,291 INFO : 164570 features and 48215 rows for ./output/github_cyber_2022-07-17T13h06m17s__1w_spaced_2d_smallest_6.0_50log.dat\n",
      "2022-07-17 13:06:18,313 INFO : Rapport = (1, 'w', 2, 'd', 21600, 50, True) :  rows=48215, lines=164570, file=./output/github_cyber_2022-07-17T13h06m17s__1w_spaced_2d_smallest_6.0_50log.dat\n",
      "\n",
      "2022-07-17 13:06:18,422 INFO : Arg : (1, 'w', 2, 'd', 86400, -1, False)\n",
      "2022-07-17 13:06:19,152 INFO : 90419 columns for the sparse data\n",
      "2022-07-17 13:06:19,990 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b28e3c824d0449781491ccfc9bcbfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad1a9fdae9d4d97a5f01e3627bec2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 13:14:00,185 INFO : 90418 features and 155567 rows for ./output/github_cyber_2022-07-17T13h13m59s__1w_spaced_2d_smallest_24.0.dat\n",
      "2022-07-17 13:14:00,235 INFO : Rapport = (1, 'w', 2, 'd', 86400, -1, False) :  rows=155567, lines=90418, file=./output/github_cyber_2022-07-17T13h13m59s__1w_spaced_2d_smallest_24.0.dat\n",
      "\n",
      "2022-07-17 13:14:00,340 INFO : Arg : (1, 'w', 2, 'd', 86400, 50, False)\n",
      "2022-07-17 13:14:00,596 INFO : From 599881 to 75850 samples\n",
      "2022-07-17 13:14:00,695 INFO : 14987 columns for the sparse data\n",
      "2022-07-17 13:14:00,784 INFO : Latest commit found at 2022-04-19 22:04:06+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89ac2f9a7c349528c395ce10c72b3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ee53cc2a1344a4a3ed9ba65130782b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 13:14:49,742 INFO : 14985 features and 14350 rows for ./output/github_cyber_2022-07-17T13h14m49s__1w_spaced_2d_smallest_24.0_50uni.dat\n",
      "2022-07-17 13:14:49,755 INFO : Rapport = (1, 'w', 2, 'd', 86400, 50, False) :  rows=14350, lines=14985, file=./output/github_cyber_2022-07-17T13h14m49s__1w_spaced_2d_smallest_24.0_50uni.dat\n",
      "\n",
      "2022-07-17 13:14:49,857 INFO : Arg : (1, 'w', 2, 'd', 86400, 50, True)\n",
      "2022-07-17 13:14:50,158 INFO : From 599881 to 349190 samples\n",
      "2022-07-17 13:14:50,494 INFO : 41146 columns for the sparse data\n",
      "2022-07-17 13:14:50,916 INFO : Latest commit found at 2022-04-23 16:44:19+00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6913f9a29a64d76a4b870396c25d819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Time window loop:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17585aa41df94e1abb60da6c83463529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repository loop:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 13:16:20,277 INFO : 41145 features and 48215 rows for ./output/github_cyber_2022-07-17T13h16m19s__1w_spaced_2d_smallest_24.0_50log.dat\n",
      "2022-07-17 13:16:20,297 INFO : Rapport = (1, 'w', 2, 'd', 86400, 50, True) :  rows=48215, lines=41145, file=./output/github_cyber_2022-07-17T13h16m19s__1w_spaced_2d_smallest_24.0_50log.dat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                    handlers=[logging.FileHandler(\"my_log.log\", mode='w'),\n",
    "                              stream_handler],\n",
    "                   force=True)\n",
    "\n",
    "\n",
    "tmd_i = [1]  # number of tmd_t\n",
    "tmd_t = ['w']  # type of tmd_i\n",
    "\n",
    "# Definition of time delta between two time window\n",
    "td_i = [1,2]  # nomber of td_t\n",
    "td_t = ['d']  # type of td_i\n",
    "\n",
    "smallest_time = [6*3600, 24*3600] # 6 hours in seconds\n",
    "\n",
    "sampling = [-1, 50]\n",
    "log = [False, True]\n",
    "\n",
    "args = list(itertools.product(*[tmd_i, tmd_t, td_i, td_t, smallest_time, sampling, log]))\n",
    "\n",
    "output_dir = \"./output\"\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss\")\n",
    "\n",
    "for arg in tqdm(args, desc=\"List of arguments\", position=0):\n",
    "    if arg[5] == -1 and arg[6]:\n",
    "        continue\n",
    "        \n",
    "    df = df_commits.copy()\n",
    "    \n",
    "    logging.info(f\"Arg : {arg}\")\n",
    "    rapport = f\"{arg} : \"\n",
    "    suffix=\"\"\n",
    "    if arg[5] > 0 :\n",
    "        dfs = get_sampling(df, arg[5], log=arg[6])\n",
    "        suffix=f\"_{arg[5]}\"\n",
    "        if arg[6]:\n",
    "            suffix += \"log\"\n",
    "        else:\n",
    "            suffix += 'uni'  \n",
    "    else :\n",
    "        dfs = df.copy()\n",
    "    rows, lines, file_name = from_commit_repo_by_week(df=dfs,\n",
    "                                         tmd_i=arg[0], \n",
    "                                         tmd_t=arg[1], \n",
    "                                         td_i=arg[2], \n",
    "                                         td_t=arg[3], \n",
    "                                         smallest_time=arg[4], \n",
    "                                         output_dir=output_dir ,\n",
    "                                         suffix=suffix)\n",
    "    rapport += f\" rows={rows}, lines={lines}, file={file_name}\\n\"\n",
    "    logging.info(f\"Rapport = {rapport}\")\n",
    "    with open(os.path.join(output_dir, f\"rapport_{today}.txt\"), 'a+') as fd:\n",
    "              fd.write(rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f13ab-9af5-4ddc-8d2e-a2f094b6d2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
